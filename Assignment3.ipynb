{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3xQa6VlNAOe"
   },
   "source": [
    "# Assignment 3\n",
    "## s3843790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "0mG3n0qRNTSA",
    "outputId": "cb0eadbd-b8a8-41b1-e3b4-7f44b47a8804"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   user_id  item_id  rating  timestamp\n0      196      242       3  881250949\n1      186      302       3  891717742\n2       22      377       1  878887116\n3      244       51       2  880606923\n4      166      346       1  886397596",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>242</td>\n      <td>3</td>\n      <td>881250949</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>186</td>\n      <td>302</td>\n      <td>3</td>\n      <td>891717742</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22</td>\n      <td>377</td>\n      <td>1</td>\n      <td>878887116</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>244</td>\n      <td>51</td>\n      <td>2</td>\n      <td>880606923</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>166</td>\n      <td>346</td>\n      <td>1</td>\n      <td>886397596</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load MovieLens 100K dataset into a dataframe of pandas\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('ml-100k/u.data', sep='\\t', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hFJT0QWudp1P"
   },
   "outputs": [],
   "source": [
    "# Select 500 most active users and 500 most active items from the dataset\n",
    "n_most_active_users = 500\n",
    "n_most_active_items = 500\n",
    "\n",
    "user_ids = df.groupby('user_id').count().sort_values(by='rating', ascending=False).head(n_most_active_users).index\n",
    "item_ids = df.groupby('item_id').count().sort_values(by='rating', ascending=False).head(n_most_active_items).index\n",
    "df = df[(df['user_id'].isin(user_ids)) & (df['item_id'].isin(item_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r012fu0jJkJc"
   },
   "outputs": [],
   "source": [
    "# Map new internal ID for items\n",
    "i_ids = df['item_id'].unique().tolist()\n",
    "item_dict = dict(zip(i_ids, [i for i in range(len(i_ids))]))\n",
    "df['item_id'] = df['item_id'].map(item_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ3rlC7jO6WJ"
   },
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwS00lvHO-ca",
    "outputId": "f9f3a031-b4e7-439e-c3e3-c165b2286d19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-2ff1e0f0649f>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['user_id'] = train_df['user_id'].map(user_dict)\n",
      "<ipython-input-4-2ff1e0f0649f>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remain_df['user_id'] = remain_df['user_id'].map(user_dict)\n"
     ]
    }
   ],
   "source": [
    "# The number of training users and active users\n",
    "n_training_users = 300\n",
    "n_active_users = n_most_active_users - n_training_users\n",
    "\n",
    "# The number of GIVEN ratings for active users\n",
    "GIVEN = 20\n",
    "\n",
    "# Randomly select users from the most active users as training set\n",
    "random_uids = np.random.choice(df.user_id.unique(), n_training_users, replace=False)\n",
    "train_df = df[df['user_id'].isin(random_uids)]\n",
    "# Map new internal ID for all users in the training set\n",
    "u_ids = train_df['user_id'].unique().tolist()\n",
    "user_dict = dict(zip(u_ids, [i for i in range(len(u_ids))]))\n",
    "train_df['user_id'] = train_df['user_id'].map(user_dict)\n",
    "\n",
    "# The rest of users are active users for testing\n",
    "remain_df = df[~df['user_id'].isin(random_uids)]\n",
    "# Map new internal ID for all active users\n",
    "u_ids = remain_df['user_id'].unique().tolist()\n",
    "user_dict = dict(zip(u_ids, [i for i in range(len(u_ids))]))\n",
    "remain_df['user_id'] = remain_df['user_id'].map(user_dict)\n",
    "\n",
    "# Randomly select GIVEN ratings for active users\n",
    "active_df = remain_df.groupby('user_id').sample(n=GIVEN, random_state=1024)\n",
    "\n",
    "test_df = remain_df[~remain_df.index.isin(active_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-ke62G3jiYb",
    "outputId": "1c135984-edb4-4f2b-fc73-9225d86a0c38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n user_id                                                    ...                  \n 0        0.0  0.0  4.0  4.0  4.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  0.0   \n 1        4.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n 2        4.0  0.0  5.0  0.0  0.0  3.0  4.0  2.0  0.0  2.0  ...  0.0  2.0  0.0   \n 3        4.0  0.0  5.0  0.0  1.0  0.0  3.0  2.0  0.0  0.0  ...  0.0  0.0  0.0   \n 4        3.0  0.0  4.0  0.0  0.0  3.0  2.0  2.0  0.0  5.0  ...  0.0  4.0  0.0   \n ...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n 295      0.0  0.0  5.0  4.0  0.0  1.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0   \n 296      4.0  0.0  3.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n 297      0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n 298      0.0  0.0  0.0  3.0  0.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  5.0   \n 299      0.0  0.0  0.0  0.0  0.0  0.0  0.0  3.0  3.0  0.0  ...  4.0  0.0  0.0   \n \n item_id  493  494  495  496  497  498  499  \n user_id                                     \n 0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 2        0.0  0.0  0.0  0.0  4.0  0.0  0.0  \n 3        4.0  1.0  0.0  0.0  0.0  0.0  2.0  \n 4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n ...      ...  ...  ...  ...  ...  ...  ...  \n 295      0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n 296      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 297      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 298      0.0  0.0  3.0  0.0  0.0  0.0  0.0  \n 299      5.0  0.0  0.0  3.0  0.0  0.0  0.0  \n \n [300 rows x 500 columns],\n item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n user_id                                                    ...                  \n 0        3.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n 1        0.0  0.0  0.0  0.0  0.0  0.0  4.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n 2        0.0  0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n 3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n 4        0.0  0.0  0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  ...  3.0  0.0  0.0   \n ...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n 195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  ...  0.0  0.0  0.0   \n 196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n 197      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  4.0   \n 198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n 199      0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  4.0  0.0   \n \n item_id  493  494  495  496  497  498  499  \n user_id                                     \n 0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n ...      ...  ...  ...  ...  ...  ...  ...  \n 195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 197      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n \n [200 rows x 500 columns],\n item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n user_id                                                    ...                  \n 0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  0.0   \n 1        0.0  2.0  0.0  4.0  0.0  4.0  0.0  0.0  0.0  2.0  ...  0.0  4.0  4.0   \n 2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  ...  0.0  0.0  0.0   \n 3        0.0  0.0  0.0  0.0  5.0  5.0  0.0  3.0  5.0  0.0  ...  0.0  4.0  0.0   \n 4        0.0  0.0  0.0  0.0  0.0  0.0  4.0  4.0  0.0  5.0  ...  0.0  0.0  0.0   \n ...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n 195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.0  0.0  0.0  ...  4.0  0.0  0.0   \n 196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n 197      4.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  0.0   \n 198      0.0  0.0  5.0  0.0  0.0  0.0  4.0  0.0  0.0  4.0  ...  0.0  0.0  0.0   \n 199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  3.0  0.0  0.0   \n \n item_id  493  494  495  496  497  498  499  \n user_id                                     \n 0        0.0  0.0  0.0  4.0  0.0  3.0  0.0  \n 1        0.0  3.0  3.0  0.0  0.0  0.0  0.0  \n 2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 3        0.0  0.0  0.0  3.0  0.0  3.0  0.0  \n 4        0.0  5.0  0.0  0.0  0.0  0.0  0.0  \n ...      ...  ...  ...  ...  ...  ...  ...  \n 195      0.0  0.0  0.0  0.0  5.0  0.0  0.0  \n 196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 197      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n 199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n \n [200 rows x 500 columns])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the format of datasets to matrices\n",
    "df_zeros = pd.DataFrame({'user_id': np.tile(np.arange(0, n_training_users), n_most_active_items), 'item_id': np.repeat(np.arange(0, n_most_active_items), n_training_users), 'rating': 0})\n",
    "train_ds = df_zeros.merge(train_df, how='left', on=['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', index='user_id', columns='item_id')\n",
    "\n",
    "df_zeros = pd.DataFrame({'user_id': np.tile(np.arange(0, n_active_users), n_most_active_items), 'item_id': np.repeat(np.arange(0, n_most_active_items), n_active_users), 'rating': 0})\n",
    "active_ds = df_zeros.merge(active_df, how='left', on=['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', index='user_id', columns='item_id')\n",
    "test_ds = df_zeros.merge(test_df, how='left', on=['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', index='user_id', columns='item_id')\n",
    "\n",
    "train_ds, active_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yElYv2TDKKGu"
   },
   "outputs": [],
   "source": [
    "# Predicting All Missing Data in training set\n",
    "imputed_train_ds = train_ds.values.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy4FurbHD4dt"
   },
   "source": [
    "# Your implementation to predict the missing values\n",
    "(Put all your implementation for your algorithm in the following cell only to handle the missing values; )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zYh1bVd0ncz3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-6f662bf887d1>:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return((min(sum(inters),GAMMA)/GAMMA)*np.sum(ra*ru)/(np.sqrt(np.sum(ra*ra))*np.sqrt(np.sum(ru*ru))))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-7-6f662bf887d1>:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return((min(sum(inters),DELTA)/DELTA)*np.sum(ri*rj)/(np.sqrt(np.sum(ri*ri))*np.sqrt(np.sum(rj*rj))))\n"
     ]
    }
   ],
   "source": [
    "## Put all your implementation for your solution in this cell only to predict the missing values; \n",
    "## NOTE 1: DO NOT change anything in the rest of the cells in this framework, \n",
    "## otherwise the changes might cause errors and make your implementation invalid.\n",
    "\n",
    "## Note 2: \n",
    "## The user-item rating matrix is imputed_train_ds, \n",
    "## and the missing values are those 0s in imputed_train_ds. \n",
    "## You are required to predict them by using the solution in the given report. \n",
    "\n",
    "## The following parameters are required in the given report, \n",
    "## which is named \"Effective Missing Data Prediction for Collaborative Filtering\", \n",
    "## and you will need to use them. But, please do not change their values. \n",
    "LAMBDA = 0.7    # λ\n",
    "GAMMA = 10      # γ\n",
    "DELTA = 10      # δ\n",
    "ITA = 0.7       # η\n",
    "THETA = 0.7     # θ\n",
    "EPSILON = 1e-9\n",
    "imputed_train_ds_copy=imputed_train_ds.copy()\n",
    "\n",
    "def Sim_U(a,u,GAMMA):\n",
    "    U_a=imputed_train_ds[a,:] #extract the data vector of a 'th' user\n",
    "    U_u=imputed_train_ds[u,:] #extract the data vector of u 'th' user\n",
    "    inters=U_a*U_u!=0 # intersection of above two sets\n",
    "    U_a=U_a[inters] # select common items\n",
    "    mean_ra=np.mean(U_a)\n",
    "    ra=U_a-mean_ra\n",
    "    U_u=U_u[inters]# select common items\n",
    "    mean_ru=np.mean(U_u)\n",
    "    ru=U_u-mean_ru\n",
    "    return((min(sum(inters),GAMMA)/GAMMA)*np.sum(ra*ru)/(np.sqrt(np.sum(ra*ra))*np.sqrt(np.sum(ru*ru))))\n",
    "\n",
    "def Sim_I(i,j,DELTA):\n",
    "    I_i=imputed_train_ds[:,i] #extract the data vector of i 'th' item\n",
    "    I_j=imputed_train_ds[:,j] #extract the data vector of j 'th' item\n",
    "    inters=I_i*I_j!=0 # intersection of above two set\n",
    "    I_i=I_i[inters] # select common users\n",
    "    mean_ri=np.mean(I_i)\n",
    "    ri=I_i-mean_ri\n",
    "    I_j=I_j[inters] # select common users\n",
    "    mean_rj=np.mean(I_j)\n",
    "    rj=I_j-mean_rj\n",
    "    return((min(sum(inters),DELTA)/DELTA)*np.sum(ri*rj)/(np.sqrt(np.sum(ri*ri))*np.sqrt(np.sum(rj*rj))))\n",
    "\n",
    "## Find similar users\n",
    "def Sim_users(u_indx,GAMMA,ITA):\n",
    "    indx_array=[]\n",
    "    for i in range(imputed_train_ds.shape[0]):\n",
    "        if i!=u_indx:\n",
    "            if Sim_U(i,u_indx,GAMMA)>ITA: #Check the condition\n",
    "                indx_array.append(i)\n",
    "    return(indx_array)\n",
    "\n",
    "## Find similar items\n",
    "def Sim_items(i_indx,DELTA,THETA):\n",
    "    indx_array=[]\n",
    "    for i in range(imputed_train_ds.shape[1]):\n",
    "        if i!=i_indx:\n",
    "            if Sim_I(i,i_indx,DELTA)>THETA: #Check the condition\n",
    "                indx_array.append(i)\n",
    "    return(indx_array)\n",
    "\n",
    "def P_u(u_indx,i_indx,GAMMA):\n",
    "    S_u=Sim_users_array[u_indx] # Find similar users\n",
    "    Sum_numerator=0\n",
    "    Sum_denominator=0\n",
    "    for u in S_u:\n",
    "        Sum_numerator+=Sim_U(u_indx,u,GAMMA)*(imputed_train_ds[u,i_indx]-np.mean(imputed_train_ds[u,:]))# Find the sum of numerator\n",
    "        Sum_denominator+=Sim_U(u_indx,u,GAMMA) # Find the sum of denominator\n",
    "    return(np.mean(imputed_train_ds[u_indx,:])+(Sum_numerator/Sum_denominator))\n",
    "\n",
    "def P_i(u_indx,i_indx,DELTA):\n",
    "    S_i=Sim_items_array[i_indx] # Find similar items\n",
    "    Sum_numerator=0\n",
    "    Sum_denominator=0\n",
    "    for i in S_i:\n",
    "        Sum_numerator+=Sim_I(i_indx,i,DELTA)*(imputed_train_ds[u_indx,i]-np.mean(imputed_train_ds[:,i])) # Find the sum of numerator\n",
    "        Sum_denominator+=Sim_I(i_indx,i,DELTA) # Find the sum of denominator\n",
    "    return(np.mean(imputed_train_ds[:,i_indx])+(Sum_numerator/Sum_denominator))\n",
    "\n",
    "## Main function to calculate missing values\n",
    "def P_ui(u_indx,i_indx,LAMBDA,DELTA,GAMMA):\n",
    "    Similar_users=Sim_users_array[u_indx]\n",
    "    Similar_items=Sim_items_array[i_indx]\n",
    "    if len(Similar_users)==0 and len(Similar_items)==0:\n",
    "        P=0\n",
    "    elif len(Similar_users)==0:\n",
    "        P=P_i(u_indx,i_indx,DELTA)\n",
    "    elif len(Similar_items)==0:\n",
    "        P=P_u(u_indx,i_indx,GAMMA)\n",
    "    else:\n",
    "        P=LAMBDA*P_u(u_indx,i_indx,GAMMA)+(1-LAMBDA)*P_i(u_indx,i_indx,DELTA)\n",
    "    return(P)\n",
    "\n",
    "## Find similar users for each individual user\n",
    "Sim_users_array=[]\n",
    "for u_indx in range(imputed_train_ds.shape[0]):\n",
    "        Sim_users_array.append(Sim_users(u_indx,GAMMA,ITA)) #append similar users to an array\n",
    "\n",
    "## Find similar items for each individual item\n",
    "Sim_items_array=[]\n",
    "for i_indx in range(imputed_train_ds.shape[1]):\n",
    "        Sim_items_array.append(Sim_items(i_indx,DELTA,THETA)) #append similar items to an array\n",
    "\n",
    "for u_indx in range(imputed_train_ds.shape[0]):\n",
    "    for i_indx in range(imputed_train_ds.shape[1]):\n",
    "        if imputed_train_ds[u_indx,i_indx]==0:\n",
    "            imputed_train_ds_copy[u_indx,i_indx]=P_ui(u_indx,i_indx,LAMBDA,DELTA,GAMMA) #update missing values\n",
    "    # print(str(u_indx+1)+\"/\"+str(imputed_train_ds.shape[0])+\" completed\")  # used to check that program works & didn't\n",
    "    # freeze since this process takes a very long time. Not necessary for the code to run properly.\n",
    "imputed_train_ds=imputed_train_ds_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbOfVWTV_Aij"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txs8YjwTzuSP"
   },
   "source": [
    "### Compute Pearson Correlation Coefficient of All Pairs of Items between active set and imputed training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "KoOgX_axKKGw",
    "outputId": "37a7adbd-e0d6-4375-e28c-3940977896f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1         2      3         4         5         6    \\\n0    0.709742  0.185840  4.000000  4.000  4.000000  0.000000  0.615872   \n1    4.000000 -0.120412  0.842319  2.000  0.279745  1.191273  1.288264   \n2    4.000000  0.185840  5.000000  0.000 -0.065279  3.000000  4.000000   \n3    4.000000  0.185840  5.000000  0.000  1.000000  0.000000  3.000000   \n4    3.000000  0.185840  4.000000  0.000 -0.065279  3.000000  2.000000   \n..        ...       ...       ...    ...       ...       ...       ...   \n295  0.709742  0.503261  5.000000  4.000 -0.065279  1.000000  0.615872   \n296  4.000000  0.288152  3.000000  0.332  0.688309  0.332000  0.417162   \n297  0.177945  0.020774  0.288023  5.000  0.579429 -0.049968  0.149784   \n298  0.709742  0.185840  1.076667  3.000 -0.065279  0.000000  0.615872   \n299  3.788781  0.475031  0.397200  4.106  0.054616  0.106000  0.722320   \n\n          7         8         9    ...       490       491       492  \\\n0    4.000000  0.713909  0.000000  ...  0.436710 -0.290000 -0.397068   \n1    1.724578 -0.215410  3.267755  ... -0.045155  0.386986 -0.295284   \n2    2.000000 -0.130822  2.000000  ...  0.478999  2.000000  0.631335   \n3    2.000000  0.661972  0.000000  ... -0.142872  3.710000 -0.397068   \n4    2.000000  0.076659  5.000000  ...  0.366388  4.000000 -0.072927   \n..        ...       ...       ...  ...       ...       ...       ...   \n295  0.986667 -0.130822  1.000000  ...  0.356034 -0.290000 -0.397068   \n296  1.428400  0.193153  0.332000  ...  0.189538  0.145400  0.627481   \n297  2.121520  0.179195  2.607886  ...  0.294261 -0.121977  1.476403   \n298  4.000000  0.291544  0.000000  ... -0.142872 -0.290000  5.000000   \n299  3.000000  3.000000  0.106000  ...  4.000000 -0.012800  0.263601   \n\n          493       494       495       496       497       498       499  \n0   -0.085740 -0.275216 -0.170000 -0.637992  0.497573  0.970816 -0.324074  \n1    0.123189  0.335312  0.269572 -0.030168  0.795629 -0.348324 -0.260279  \n2   -0.085740  0.858389 -0.170000 -0.399509  4.000000  0.073221  0.090053  \n3    4.000000  1.000000 -0.170000 -0.637992  2.340809 -0.573869  2.000000  \n4    2.096627  0.582543 -0.170000  0.173373  0.302641 -0.573869  0.337994  \n..        ...       ...       ...       ...       ...       ...       ...  \n295 -0.085740  0.460026  0.830000  0.149024 -0.272173  1.000000  0.874082  \n296  0.206678  0.484721  2.281400  0.376466  0.317055  0.060239  0.128226  \n297  0.484591 -0.117542  1.030321  1.493025 -0.116629 -0.207138 -0.118913  \n298 -0.085740  0.131415  3.000000 -0.280022  0.097397  0.018340 -0.835702  \n299  5.000000  0.338005  0.023200  3.000000  0.214290  0.198144  0.262648  \n\n[300 rows x 500 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>490</th>\n      <th>491</th>\n      <th>492</th>\n      <th>493</th>\n      <th>494</th>\n      <th>495</th>\n      <th>496</th>\n      <th>497</th>\n      <th>498</th>\n      <th>499</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.709742</td>\n      <td>0.185840</td>\n      <td>4.000000</td>\n      <td>4.000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>0.615872</td>\n      <td>4.000000</td>\n      <td>0.713909</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.436710</td>\n      <td>-0.290000</td>\n      <td>-0.397068</td>\n      <td>-0.085740</td>\n      <td>-0.275216</td>\n      <td>-0.170000</td>\n      <td>-0.637992</td>\n      <td>0.497573</td>\n      <td>0.970816</td>\n      <td>-0.324074</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.000000</td>\n      <td>-0.120412</td>\n      <td>0.842319</td>\n      <td>2.000</td>\n      <td>0.279745</td>\n      <td>1.191273</td>\n      <td>1.288264</td>\n      <td>1.724578</td>\n      <td>-0.215410</td>\n      <td>3.267755</td>\n      <td>...</td>\n      <td>-0.045155</td>\n      <td>0.386986</td>\n      <td>-0.295284</td>\n      <td>0.123189</td>\n      <td>0.335312</td>\n      <td>0.269572</td>\n      <td>-0.030168</td>\n      <td>0.795629</td>\n      <td>-0.348324</td>\n      <td>-0.260279</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.000000</td>\n      <td>0.185840</td>\n      <td>5.000000</td>\n      <td>0.000</td>\n      <td>-0.065279</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>2.000000</td>\n      <td>-0.130822</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>0.478999</td>\n      <td>2.000000</td>\n      <td>0.631335</td>\n      <td>-0.085740</td>\n      <td>0.858389</td>\n      <td>-0.170000</td>\n      <td>-0.399509</td>\n      <td>4.000000</td>\n      <td>0.073221</td>\n      <td>0.090053</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.000000</td>\n      <td>0.185840</td>\n      <td>5.000000</td>\n      <td>0.000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>0.661972</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-0.142872</td>\n      <td>3.710000</td>\n      <td>-0.397068</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>-0.170000</td>\n      <td>-0.637992</td>\n      <td>2.340809</td>\n      <td>-0.573869</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.000000</td>\n      <td>0.185840</td>\n      <td>4.000000</td>\n      <td>0.000</td>\n      <td>-0.065279</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>0.076659</td>\n      <td>5.000000</td>\n      <td>...</td>\n      <td>0.366388</td>\n      <td>4.000000</td>\n      <td>-0.072927</td>\n      <td>2.096627</td>\n      <td>0.582543</td>\n      <td>-0.170000</td>\n      <td>0.173373</td>\n      <td>0.302641</td>\n      <td>-0.573869</td>\n      <td>0.337994</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>0.709742</td>\n      <td>0.503261</td>\n      <td>5.000000</td>\n      <td>4.000</td>\n      <td>-0.065279</td>\n      <td>1.000000</td>\n      <td>0.615872</td>\n      <td>0.986667</td>\n      <td>-0.130822</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.356034</td>\n      <td>-0.290000</td>\n      <td>-0.397068</td>\n      <td>-0.085740</td>\n      <td>0.460026</td>\n      <td>0.830000</td>\n      <td>0.149024</td>\n      <td>-0.272173</td>\n      <td>1.000000</td>\n      <td>0.874082</td>\n    </tr>\n    <tr>\n      <th>296</th>\n      <td>4.000000</td>\n      <td>0.288152</td>\n      <td>3.000000</td>\n      <td>0.332</td>\n      <td>0.688309</td>\n      <td>0.332000</td>\n      <td>0.417162</td>\n      <td>1.428400</td>\n      <td>0.193153</td>\n      <td>0.332000</td>\n      <td>...</td>\n      <td>0.189538</td>\n      <td>0.145400</td>\n      <td>0.627481</td>\n      <td>0.206678</td>\n      <td>0.484721</td>\n      <td>2.281400</td>\n      <td>0.376466</td>\n      <td>0.317055</td>\n      <td>0.060239</td>\n      <td>0.128226</td>\n    </tr>\n    <tr>\n      <th>297</th>\n      <td>0.177945</td>\n      <td>0.020774</td>\n      <td>0.288023</td>\n      <td>5.000</td>\n      <td>0.579429</td>\n      <td>-0.049968</td>\n      <td>0.149784</td>\n      <td>2.121520</td>\n      <td>0.179195</td>\n      <td>2.607886</td>\n      <td>...</td>\n      <td>0.294261</td>\n      <td>-0.121977</td>\n      <td>1.476403</td>\n      <td>0.484591</td>\n      <td>-0.117542</td>\n      <td>1.030321</td>\n      <td>1.493025</td>\n      <td>-0.116629</td>\n      <td>-0.207138</td>\n      <td>-0.118913</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>0.709742</td>\n      <td>0.185840</td>\n      <td>1.076667</td>\n      <td>3.000</td>\n      <td>-0.065279</td>\n      <td>0.000000</td>\n      <td>0.615872</td>\n      <td>4.000000</td>\n      <td>0.291544</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-0.142872</td>\n      <td>-0.290000</td>\n      <td>5.000000</td>\n      <td>-0.085740</td>\n      <td>0.131415</td>\n      <td>3.000000</td>\n      <td>-0.280022</td>\n      <td>0.097397</td>\n      <td>0.018340</td>\n      <td>-0.835702</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>3.788781</td>\n      <td>0.475031</td>\n      <td>0.397200</td>\n      <td>4.106</td>\n      <td>0.054616</td>\n      <td>0.106000</td>\n      <td>0.722320</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>0.106000</td>\n      <td>...</td>\n      <td>4.000000</td>\n      <td>-0.012800</td>\n      <td>0.263601</td>\n      <td>5.000000</td>\n      <td>0.338005</td>\n      <td>0.023200</td>\n      <td>3.000000</td>\n      <td>0.214290</td>\n      <td>0.198144</td>\n      <td>0.262648</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows × 500 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_train_ds = pd.DataFrame(imputed_train_ds)\n",
    "imputed_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq0uq1aHzu11",
    "outputId": "f6214a46-d63e-4fdc-e7dd-ccaed23b237d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.38303595, -0.23134493, -0.09747023, ..., -0.15247848,\n        -0.20815069, -0.65427675],\n       [-0.18746657, -0.00124785,  0.29575709, ..., -0.44828989,\n        -0.80511005, -0.15298635],\n       [ 0.14528534,  0.20069997,  0.60650224, ...,  0.39705794,\n         0.50926306, -0.1200081 ],\n       ...,\n       [ 0.16916139,  0.13498877,  0.36317678, ...,  0.01153526,\n         0.29426612,  0.04629905],\n       [ 0.3238444 ,  0.47840437,  0.19637531, ...,  0.15209106,\n        -0.09605882,  0.33775953],\n       [ 0.13064772,  0.2434322 ,  0.26074242, ...,  0.36882501,\n         0.27123087,  0.13553229]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_user_pearson_corr = np.zeros((active_ds.shape[0], train_ds.shape[0]))\n",
    "\n",
    "# Compute Pearson Correlation Coefficient of All Pairs of Users between active set and imputed training set\n",
    "for i, user_i_vec in enumerate(active_ds.values):\n",
    "    for j, user_j_vec in enumerate(imputed_train_ds.values):\n",
    "        \n",
    "        # ratings corated by the current pair od users\n",
    "        mask_i = user_i_vec > 0\n",
    "        mask_j = user_j_vec > 0\n",
    "\n",
    "        # corrated item index, skip if there are no corrated ratings\n",
    "        corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "        if len(corrated_index) == 0:\n",
    "            continue\n",
    "\n",
    "        # average value of user_i_vec and user_j_vec\n",
    "        mean_user_i = np.sum(user_i_vec) / (np.sum(np.clip(user_i_vec, 0, 1)) + EPSILON)\n",
    "        mean_user_j = np.sum(user_j_vec) / (np.sum(np.clip(user_j_vec, 0, 1)) + EPSILON)\n",
    "\n",
    "        # compute pearson corr\n",
    "        user_i_sub_mean = user_i_vec[corrated_index] - mean_user_i\n",
    "        user_j_sub_mean = user_j_vec[corrated_index] - mean_user_j\n",
    "\n",
    "        r_ui_sub_r_i_sq = np.square(user_i_sub_mean)\n",
    "        r_uj_sub_r_j_sq = np.square(user_j_sub_mean)\n",
    "\n",
    "        r_ui_sum_sqrt = np.sqrt(np.sum(r_ui_sub_r_i_sq))\n",
    "        r_uj_sum_sqrt = np.sqrt(np.sum(r_uj_sub_r_j_sq))\n",
    "\n",
    "        sim = np.sum(user_i_sub_mean * user_j_sub_mean) / (r_ui_sum_sqrt * r_uj_sum_sqrt + EPSILON)\n",
    "\n",
    "        # significance weighting\n",
    "        weighted_sim = (min(len(corrated_index), GAMMA) / GAMMA) * sim\n",
    "\n",
    "        active_user_pearson_corr[i][j] = weighted_sim\n",
    "\n",
    "active_user_pearson_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewTnN9kNb8Ys"
   },
   "source": [
    "## Predict Ratings of Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4ERndYXb8Ys",
    "outputId": "12607670-af61-403a-e4ce-f5e874bf8386"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        , ..., 0.        , 2.39155812,\n        0.        ],\n       [0.        , 2.15099917, 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [5.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 5.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "\n",
    "test_ds_pred = np.zeros_like(test_ds.values)\n",
    "\n",
    "for (i, j), rating in np.ndenumerate(test_ds.values):\n",
    "\n",
    "    if rating > 0:\n",
    "\n",
    "        sim_user_ids = np.argsort(active_user_pearson_corr[i])[-1:-(K + 1):-1]\n",
    "\n",
    "        #==================user-based==================#\n",
    "        # the coefficient values of similar users\n",
    "        sim_val = active_user_pearson_corr[i][sim_user_ids]\n",
    "\n",
    "        # the average value of the current user's ratings\n",
    "        sim_users = imputed_train_ds.values[sim_user_ids]\n",
    "        user_mean = np.sum(active_ds.values[i]) / (np.sum(np.clip(active_ds.values[i], 0, 1)) + EPSILON)\n",
    "        sim_user_mean = np.sum(sim_users, axis=1) / (np.sum(np.clip(sim_users, 0, 1), axis=1) + EPSILON)\n",
    "\n",
    "        # select the users who rated item j\n",
    "        mask_rated_j = sim_users[:, j] > 0\n",
    "        \n",
    "        # sim(u, v) * (r_vj - mean_v)\n",
    "        sim_r_sum_mean = sim_val[mask_rated_j] * (sim_users[mask_rated_j, j] - sim_user_mean[mask_rated_j])\n",
    "        \n",
    "        user_based_pred = user_mean + np.sum(sim_r_sum_mean) / (np.sum(sim_val[mask_rated_j]) + EPSILON)\n",
    "        user_based_pred = np.clip(user_based_pred, 0, 5)\n",
    "\n",
    "        test_ds_pred[i][j] = user_based_pred\n",
    "        \n",
    "test_ds_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUTn4kSFb8ZA"
   },
   "source": [
    "## Compute MAE and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JCVmexyb8ZA",
    "outputId": "6f650170-e9a6-482a-b695-5fa474d964b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.9161860989123678, RMSE: 1.1706436261408193\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "MAE = np.sum(np.abs(test_ds_pred - test_ds.values)) / np.sum(np.clip(test_ds.values, 0, 1))\n",
    "\n",
    "# RMSE\n",
    "RMSE = np.sqrt(np.sum(np.square(test_ds_pred - test_ds.values)) / np.sum(np.clip(test_ds.values, 0, 1)))\n",
    "\n",
    "print(\"MAE: {}, RMSE: {}\" .format(MAE, RMSE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}